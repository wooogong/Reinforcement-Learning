{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO5m3VyJFlNJrqukNg5wU+S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wooogong/Reinforcement-Learning/blob/main/ex_gym.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIAeSvHB3JwM"
      },
      "source": [
        "import gym\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08OwdCbb3MJ_"
      },
      "source": [
        "env = gym.make('Blackjack-v0')\n",
        "print(env.observation_space)\n",
        "print(env.action_space)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDtR-bXN3S6D"
      },
      "source": [
        "for i in range(10):\n",
        "  state = env.reset()\n",
        "  #print(state)\n",
        "  while True:\n",
        "    action = env.action_space.sample()\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    print(state, action, reward, next_state)\n",
        "\n",
        "    if done:\n",
        "      print(\"done\")\n",
        "      break\n",
        "      state = next_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSElON3i39A-"
      },
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from IPython import display\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spuPtPKO9PZ9"
      },
      "source": [
        "def random_policy(observation):\n",
        "  return env.action_space.sample()\n",
        "\n",
        "def hit_only_policy(observation):\n",
        "    if observation[0] == 21:\n",
        "      return 0\n",
        "    return 1\n",
        "\n",
        "def stick_only_policy(observation):\n",
        "  return 0\n",
        "\n",
        "def fixed_policy(observation):\n",
        "  score, dealer_score, usable_ace = observation\n",
        "  return 0 if score >= 16 else 1\n",
        "\n",
        "#V_tmp = mc_prediction(fixed_policy, env, num_episodes = 1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJMsV_dY9kne"
      },
      "source": [
        "def play_game(policy, env):\n",
        "  nWon = 0\n",
        "  nLost = 0\n",
        "  nDraw = 0\n",
        "  policy = policy\n",
        "\n",
        "  for i in range(100000):\n",
        "    observation = env.reset()\n",
        "    for i in range(100):\n",
        "      action = policy(observation)\n",
        "\n",
        "      next_observation, reward, done, _ = env.step(action)\n",
        "\n",
        "      if done:\n",
        "\n",
        "        if reward == 1:\n",
        "          nWon += 1\n",
        "        elif reward == -1:\n",
        "          nLost += 1\n",
        "        else:\n",
        "          nDraw += 1\n",
        "        break\n",
        "      observation = next_observation\n",
        "    print(nWon, nLost, nDraw)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pu3U6kST-bzR"
      },
      "source": [
        "play_game(random_policy, env)\n",
        "play_game(hit_only_policy, env)\n",
        "play_game(stick_only_policy, env)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OELOViTk-k6g"
      },
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def mc_prediction(policy, env, num_episodes, alpha = 0.01, gamma = 0.99):\n",
        "  N = defaultdict(float)\n",
        "  V = defaultdict(float)\n",
        "\n",
        "  for i_episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    trajectory = []\n",
        "    while True:\n",
        "      action = policy(state)\n",
        "      next_state, reward, done, _ = env.step(action)\n",
        "      trajectory.append((state, action, reward))\n",
        "\n",
        "      if done:\n",
        "        break\n",
        "\n",
        "      state = next_state\n",
        "\n",
        "    states, actions, rewards = zip(*trajectory)\n",
        "    discounts = np.array([gamma**i for i in range(len(rewards))])\n",
        "\n",
        "    for i, s in enumerate(states):\n",
        "      N[s] += 1\n",
        "      G = sum(rewards[i:]*discounts[:len(rewards) - 1])\n",
        "\n",
        "      V[s] = V[s] + 1/N[s] * (G-V[s])\n",
        "\n",
        "    return V\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7pUDPM0_zoG"
      },
      "source": [
        "V_random = mc_prediction(random_policy, env, num_episodes = 100000)\n",
        "#plot_value_function(V_random, tilte = '100000 Steps')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fpl_KRfhAALt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}